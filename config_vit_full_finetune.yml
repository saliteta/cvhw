# ViT (DINOv3) Full Fine-Tuning Configuration
# CIFAR-10 Image Classification

model:
  architecture: DINOv3-ViT-Large
  backbone: vit_large_patch16_dinov3
  num_classes: 10
  input_size: [3, 448, 448]
  total_parameters: 303,091,722
  trainable_parameters: 303,091,722  # All parameters trainable
  embedding_dim: 1024

dataset:
  name: CIFAR-10
  train_samples: 50000
  test_samples: 10000
  classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

hyperparameters:
  # Training settings
  batch_size: 8
  gradient_accumulation_steps: 8
  effective_batch_size: 64  # batch_size * gradient_accumulation_steps
  num_epochs: 1
  num_workers: 2
  img_size: 448
  
  # Training mode
  freeze_backbone: false
  training_mode: full_finetuning
  use_gradient_checkpointing: true
  
  # Optimizer (Adam) - differential learning rates
  optimizer: Adam
  learning_rate_backbone: 1e-6  # Very small for pretrained backbone
  learning_rate_classifier: 1e-5  # Larger for classifier (10x backbone)
  beta1: 0.9            # β1
  beta2: 0.999          # β2
  weight_decay: 1e-4
  
  # Learning rate scheduler
  scheduler: MultiStepLR
  milestones: [10, 15]
  gamma: 0.1

memory_optimization:
  gradient_checkpointing: true
  gradient_accumulation: true
  mixed_precision: false

data_augmentation:
  train:
    - Resize: [448, 448]
    - RandomHorizontalFlip
    - ToTensor
    - Normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]
  test:
    - Resize: [448, 448]
    - ToTensor
    - Normalize:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

pretrained:
  source: timm
  model_name: vit_large_patch16_dinov3
  pretrained_weights: true
  strategy: full_finetuning

results:
  best_epoch: 1
  test_accuracy: 30.11%  # Poor performance - needs better hyperparameters
  test_loss: N/A
  checkpoint: checkpoints/dinov3_full_finetune_best.pth

wandb:
  project: dinov3-cifar10-full-finetune
  entity: null

notes: |
  Full fine-tuning of all 303M parameters.
  Uses gradient checkpointing to reduce memory usage.
  Gradient accumulation simulates larger batch sizes.
  CAUTION: Current hyperparameters resulted in poor performance.
  Recommended to use higher learning rate (1e-6 to 1e-5) and more epochs.

